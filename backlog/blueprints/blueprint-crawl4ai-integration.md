# Blueprint: Crawl4AI Integration

**Feature:** Web crawling via Crawl4AI sidecar service  
**Status:** Draft  
**Created:** 2026-01-16  
**Architecture:** Sidecar Docker container  

---

## Overview

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Crawl4AI –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞. Crawl4AI —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ sidecar-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∫—Ä–∞—É–ª–∏–Ω–≥–∞ JS-—Ä–µ–Ω–¥–µ—Ä–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –∏–∑–≤–ª–µ—á–µ–Ω–∏—è markdown –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ PDF.

**–ö–ª—é—á–µ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è:**
- Tavily –∏ Crawl4AI –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ (compile-time check)
- Crawl4AI –∫–∞–∫ Docker sidecar –Ω–∞ –ø–æ—Ä—Ç—É 11235
- 3 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: `deep_crawl`, `web_markdown`, `web_pdf`
- Memory limit: 4GB

---

## Phase 0: Feature Flags Setup [ ]

**Goal:** –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ features –¥–ª—è Tavily –∏ Crawl4AI.

**Resource Context:**
- üìÑ `Cargo.toml`
- üìÑ `src/agent/providers/mod.rs`

**Steps:**
1. [ ] –í `Cargo.toml` –¥–æ–±–∞–≤–∏—Ç—å feature `crawl4ai = []` (–±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π `reqwest`)
2. [ ] –í `src/agent/providers/mod.rs` –¥–æ–±–∞–≤–∏—Ç—å compile_error –¥–ª—è –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–µ–Ω–∏—è features
3. [ ] –î–æ–±–∞–≤–∏—Ç—å —É—Å–ª–æ–≤–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –º–æ–¥—É–ª—è `crawl4ai`
4. [ ] **QA:** `cargo check --features tavily` –∏ `cargo check --features crawl4ai` –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ
5. [ ] **QA:** `cargo check --features tavily,crawl4ai` –¥–æ–ª–∂–µ–Ω –≤—ã–¥–∞—Ç—å compile_error

> [!NOTE]
> Cargo –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç exclusive features –Ω–∞–ø—Ä—è–º—É—é, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º `compile_error!` –º–∞–∫—Ä–æ—Å.

---

## Phase 1: Docker Infrastructure [ ]

**Goal:** –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Crawl4AI –∫–∞–∫ sidecar-—Å–µ—Ä–≤–∏—Å –≤ docker-compose.

**Resource Context:**
- üìÑ `docker-compose.yml`
- üìÑ `.env.example`
- üìö **Docs:** Crawl4AI self-hosting guide ‚Äî `https://docs.crawl4ai.com/core/self-hosting/`

**Steps:**
1. [ ] **Verify API:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `tavily_extract` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö endpoints Crawl4AI (`/crawl`, `/md`, `/pdf`)
2. [ ] –£–±—Ä–∞—Ç—å `network_mode: "host"` –∏–∑ `oxide_agent` service
3. [ ] –î–æ–±–∞–≤–∏—Ç—å bridge network `oxide_network`
4. [ ] –î–æ–±–∞–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å `crawl4ai`:
   - image: `unclecode/crawl4ai:latest`
   - networks: `oxide_network`
   - volumes: `/dev/shm:/dev/shm`
   - memory limit: 4G
   - healthcheck –Ω–∞ `/health`
5. [ ] –î–æ–±–∞–≤–∏—Ç—å `depends_on` —Å `condition: service_healthy` –≤ `oxide_agent`
6. [ ] –î–æ–±–∞–≤–∏—Ç—å environment variable `CRAWL4AI_URL=http://crawl4ai:11235`
7. [ ] –û–±–Ω–æ–≤–∏—Ç—å `.env.example` —Å –Ω–æ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
8. [ ] **QA:** `docker compose config` –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ YAML

> [!IMPORTANT]
> –ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ network_mode —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ Docker socket –º–æ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ volume (–¥–ª—è sandbox).

**Docker Compose Structure:**
```
services:
  oxide_agent:
    networks: [oxide_network]
    depends_on:
      crawl4ai:
        condition: service_healthy
    environment:
      - CRAWL4AI_URL=http://crawl4ai:11235

  crawl4ai:
    image: unclecode/crawl4ai:latest
    networks: [oxide_network]
    volumes:
      - /dev/shm:/dev/shm
    deploy:
      resources:
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11235/health"]

networks:
  oxide_network:
    driver: bridge
```

---

## Phase 2: Configuration [ ]

**Goal:** –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Crawl4AI –≤ Settings.

**Resource Context:**
- üìÑ `src/config.rs`

**Steps:**
1. [ ] –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—è –≤ struct `Settings`:
   - `crawl4ai_url: Option<String>`
   - `crawl4ai_timeout_secs: Option<u64>`
2. [ ] –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É `CRAWL4AI_DEFAULT_TIMEOUT_SECS: u64 = 120`
3. [ ] –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_crawl4ai_url() -> Option<String>`
4. [ ] –î–æ–±–∞–≤–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_crawl4ai_timeout() -> u64`
5. [ ] **QA:** `cargo check` –±–µ–∑ –æ—à–∏–±–æ–∫

---

## Phase 3: Crawl4AI Provider Implementation [ ]

**Goal:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å 3 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.

**Resource Context:**
- üìÑ `src/agent/providers/tavily.rs` (reference implementation)
- üìÑ `src/agent/provider.rs` (ToolProvider trait)
- üìö **Docs:** Crawl4AI API endpoints:
  - `POST /crawl` ‚Äî deep crawling
  - `POST /md` ‚Äî markdown extraction  
  - `POST /pdf` ‚Äî PDF export

**Steps:**
1. [ ] **Verify API Signatures:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `tavily_search` + `tavily_extract` –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
   - Request/response format –¥–ª—è `/crawl`
   - Request/response format –¥–ª—è `/md`
   - Request/response format –¥–ª—è `/pdf`
2. [ ] –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª `src/agent/providers/crawl4ai.rs`
3. [ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å struct `Crawl4aiProvider`:
   - `base_url: String`
   - `client: reqwest::Client`
   - `timeout: Duration`
4. [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `Crawl4aiProvider::new(base_url: &str) -> Self`
5. [ ] –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å argument structs:
   - `DeepCrawlArgs { urls: Vec<String>, max_depth: Option<u8> }`
   - `WebMarkdownArgs { url: String }`
   - `WebPdfArgs { url: String }`
6. [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `ToolProvider` trait:
   - `name()` ‚Üí `"crawl4ai"`
   - `tools()` ‚Üí 3 ToolDefinition
   - `can_handle()` ‚Üí match –Ω–∞ –∏–º–µ–Ω–∞
   - `execute()` ‚Üí HTTP POST –∫ endpoints
7. [ ] –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å user-friendly —Å–æ–æ–±—â–µ–Ω–∏—è
8. [ ] **QA:** `cargo check --features crawl4ai`
9. [ ] **QA:** `cargo clippy --features crawl4ai`

> [!IMPORTANT]
> –ü–µ—Ä–µ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π `execute()` –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ API signatures —á–µ—Ä–µ–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é, —Ç–∞–∫ –∫–∞–∫ Crawl4AI –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è.

**Tool Definitions:**

| Tool | Description | Parameters |
|------|-------------|------------|
| `deep_crawl` | Deep crawl website with JS rendering | `urls: string[]`, `max_depth?: number` |
| `web_markdown` | Extract markdown from URL | `url: string` |
| `web_pdf` | Export webpage to PDF | `url: string` |

**API Request Examples:**

```json
// POST /crawl
{
  "urls": ["https://example.com"],
  "crawler_config": {
    "type": "CrawlerRunConfig",
    "params": {"cache_mode": "bypass"}
  }
}

// POST /md
{
  "url": "https://example.com",
  "f": "fit"
}

// POST /pdf
{
  "url": "https://example.com"
}
```

---

## Phase 4: Provider Registration [ ]

**Goal:** –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤ executor –∏ delegation.

**Resource Context:**
- üìÑ `src/agent/executor.rs` (lines 104-156)
- üìÑ `src/agent/providers/delegation.rs` (lines 100-120)

**Steps:**
1. [ ] –í `executor.rs` –¥–æ–±–∞–≤–∏—Ç—å import –ø–æ–¥ `#[cfg(feature = "crawl4ai")]`
2. [ ] –í `executor.rs` –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ Tavily –±–ª–æ–∫–∞:
   ```rust
   #[cfg(feature = "crawl4ai")]
   if let Ok(url) = std::env::var("CRAWL4AI_URL") {
       if !url.is_empty() {
           registry.register(Box::new(Crawl4aiProvider::new(&url)));
       }
   }
   ```
3. [ ] –í `delegation.rs` –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –≤ `build_sub_agent_registry()`
4. [ ] **QA:** `cargo check --features crawl4ai`

---

## Phase 5: Skill File Update [ ]

**Goal:** –û–±–Ω–æ–≤–∏—Ç—å skill-—Ñ–∞–π–ª –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –æ–±–æ–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.

**Resource Context:**
- üìÑ `skills/web-search.md`

**Steps:**
1. [ ] –û–±–Ω–æ–≤–∏—Ç—å `allowed_tools` ‚Äî –¥–æ–±–∞–≤–∏—Ç—å `deep_crawl`, `web_markdown`, `web_pdf`
2. [ ] –û–±–Ω–æ–≤–∏—Ç—å `triggers` ‚Äî –¥–æ–±–∞–≤–∏—Ç—å `crawl`, `extract`, `pdf`
3. [ ] –î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ Crawl4AI
4. [ ] –î–æ–±–∞–≤–∏—Ç—å guidelines –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç

**Updated Content:**
```markdown
---
name: web-search
description: Search and extract information from the internet
triggers: [find, search, look up, current, news, docs, crawl, extract, pdf]
allowed_tools: [web_search, web_extract, deep_crawl, web_markdown, web_pdf]
weight: medium
---

## Web Search & Extraction

### Quick Search (Tavily):
- **web_search**: Search internet for news, facts, documentation
- **web_extract**: Extract content from URLs

### Deep Crawling (Crawl4AI):
- **deep_crawl**: Deep crawl with JS rendering for dynamic sites
- **web_markdown**: Fast markdown extraction from single URL
- **web_pdf**: Export webpage to PDF document

## Guidelines:
- Quick facts/news ‚Üí web_search
- Read article ‚Üí web_extract or web_markdown
- JS-heavy SPA sites ‚Üí deep_crawl
- Save for later/archive ‚Üí web_pdf
```

---

## Phase 6: Testing [ ]

**Goal:** –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.

**Resource Context:**
- üìÑ `src/agent/providers/crawl4ai.rs`
- üìÑ `tests/` directory

**Steps:**
1. [ ] –î–æ–±–∞–≤–∏—Ç—å unit-—Ç–µ—Å—Ç—ã –≤ `crawl4ai.rs`:
   - Test argument deserialization
   - Test URL construction
   - Test error message formatting
2. [ ] –°–æ–∑–¥–∞—Ç—å `tests/crawl4ai_provider.rs`:
   - Test `can_handle()` –¥–ª—è –≤—Å–µ—Ö 3 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
   - Test `tools()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 3 –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
3. [ ] **QA:** `cargo test --features crawl4ai`

---

## Phase 7: Documentation [ ]

**Goal:** –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞.

**Resource Context:**
- üìÑ `AGENTS.md`
- üìÑ `.env.example`

**Steps:**
1. [ ] –í `AGENTS.md` –¥–æ–±–∞–≤–∏—Ç—å `crawl4ai.rs` –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É providers
2. [ ] –í `.env.example` –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø—Ä–æ –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–µ–Ω–∏–µ Tavily/Crawl4AI
3. [ ] –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è Crawl4AI

---

## Summary

| Phase | Files | Estimated LOC |
|-------|-------|---------------|
| 0. Feature Flags | 2 | ~15 |
| 1. Docker | 2 | ~35 |
| 2. Config | 1 | ~25 |
| 3. Provider | 1 (new) | ~180 |
| 4. Registration | 2 | ~20 |
| 5. Skill | 1 | ~25 |
| 6. Testing | 2 | ~60 |
| 7. Docs | 2 | ~15 |
| **Total** | **~13 files** | **~375 LOC** |

**Estimated Time:** 1.5-2 hours

---

## Risks & Mitigations

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| network_mode change breaks Docker socket | Medium | High | Verify socket mount works with bridge network |
| Crawl4AI API changes | Low | Medium | Pin image version, verify docs before impl |
| Large PDF responses | Medium | Low | Limit response size, add timeout |
| Crawl4AI cold start slow | Low | Low | healthcheck with start_period: 40s |

---

## Acceptance Criteria

- [ ] `cargo build --features crawl4ai` succeeds
- [ ] `cargo build --features tavily,crawl4ai` fails with compile_error
- [ ] `docker compose up` starts both services
- [ ] Agent can use `deep_crawl` tool successfully
- [ ] Agent can use `web_markdown` tool successfully  
- [ ] Agent can use `web_pdf` tool successfully
- [ ] All tests pass
