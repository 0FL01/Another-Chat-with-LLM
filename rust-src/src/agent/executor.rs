//! Agent executor using Rig framework
//!
//! Handles the iterative execution of tasks using Devstral model
//! with progress reporting and timeout management.

use super::memory::AgentMessage;
use super::session::AgentSession;
use crate::config::{AGENT_MODEL, AGENT_TIMEOUT_SECS};
use crate::llm::LlmClient;
use anyhow::{anyhow, Result};
use std::sync::Arc;
use tokio::sync::mpsc;
use tokio::time::{timeout, Duration};
use tracing::{debug, error, info, instrument, trace};

/// Progress update sent during task execution
#[derive(Debug, Clone)]
pub struct ProgressUpdate {
    pub step: String,
    pub progress_percent: u8,
    pub is_final: bool,
}

/// Agent executor that runs tasks iteratively
pub struct AgentExecutor {
    llm_client: Arc<LlmClient>,
    session: AgentSession,
}

impl AgentExecutor {
    /// Create a new agent executor
    pub fn new(llm_client: Arc<LlmClient>, session: AgentSession) -> Self {
        Self {
            llm_client,
            session,
        }
    }

    /// Get a reference to the session
    pub fn session(&self) -> &AgentSession {
        &self.session
    }

    /// Get a mutable reference to the session
    pub fn session_mut(&mut self) -> &mut AgentSession {
        &mut self.session
    }

    /// The final result is sent as the last update with `is_final = true`.
    #[instrument(skip(self), fields(user_id = self.session.user_id, chat_id = self.session.chat_id, task_id = %self.session.current_task_id.as_deref().unwrap_or("none")))]
    pub async fn execute_with_progress(
        &mut self,
        task: &str,
    ) -> Result<mpsc::Receiver<ProgressUpdate>> {
        // Start the task (generates task_id)
        self.session.start_task();
        let task_id = self.session.current_task_id.clone().unwrap_or_default();

        info!(task = %task, task_id = %task_id, "Starting agent task with progress reporting");
        let (tx, rx) = mpsc::channel::<ProgressUpdate>(32);

        // Add user message to memory
        self.session.memory.add_message(AgentMessage::user(task));

        // Clone what we need for the async task
        let llm_client = self.llm_client.clone();
        let task_str = task.to_string();
        let memory_messages = self.session.memory.get_messages().to_vec();

        // Spawn the execution task
        let tx_clone = tx.clone();
        let task_id_clone = task_id.clone();
        tokio::spawn(async move {
            let start = std::time::Instant::now();
            let result = Self::run_agent_loop(
                llm_client,
                task_str,
                memory_messages,
                tx_clone,
                task_id_clone.clone(),
            )
            .await;

            let duration = start.elapsed();
            match result {
                Ok(response) => {
                    info!(task_id = %task_id_clone, duration_ms = duration.as_millis(), "Agent task completed successfully");
                    let _ = tx
                        .send(ProgressUpdate {
                            step: response,
                            progress_percent: 100,
                            is_final: true,
                        })
                        .await;
                }
                Err(e) => {
                    error!(task_id = %task_id_clone, error = %e, duration_ms = duration.as_millis(), "Agent task failed");
                    let _ = tx
                        .send(ProgressUpdate {
                            step: format!("‚ùå –û—à–∏–±–∫–∞: {}", e),
                            progress_percent: 100,
                            is_final: true,
                        })
                        .await;
                }
            }
        });

        Ok(rx)
    }

    /// Execute a task with iterative tool calling (agentic loop)
    pub async fn execute(&mut self, task: &str) -> Result<String> {
        use super::tools::{execute_tool, get_agent_tools};
        use crate::llm::Message;

        const MAX_ITERATIONS: usize = 10;

        // Start the task
        self.session.start_task();
        let task_id = self.session.current_task_id.clone().unwrap_or_default();

        info!(task = %task, task_id = %task_id, "Starting agent task with tool calling");

        // Add user message to memory
        self.session.memory.add_message(AgentMessage::user(task));

        // Create the system prompt and get tools
        let system_prompt = Self::create_agent_system_prompt();
        let tools = get_agent_tools();

        // Build initial messages from memory
        let mut messages: Vec<Message> = self
            .session
            .memory
            .get_messages()
            .iter()
            .map(|msg| {
                let role = match msg.role {
                    super::memory::MessageRole::User => "user",
                    super::memory::MessageRole::Assistant => "assistant",
                    super::memory::MessageRole::System => "system",
                };
                Message {
                    role: role.to_string(),
                    content: msg.content.clone(),
                    tool_call_id: None,
                    name: None,
                }
            })
            .collect();

        // Execute with timeout
        let timeout_duration = Duration::from_secs(AGENT_TIMEOUT_SECS);

        let result = timeout(timeout_duration, async {
            // Agentic loop
            for iteration in 0..MAX_ITERATIONS {
                debug!(
                    task_id = %task_id,
                    iteration = iteration,
                    messages_count = messages.len(),
                    "Agent loop iteration"
                );

                // Call LLM with tools
                let response = self
                    .llm_client
                    .chat_with_tools(&system_prompt, &messages, &tools, AGENT_MODEL)
                    .await
                    .map_err(|e| anyhow!("LLM call failed: {}", e))?;

                debug!(
                    task_id = %task_id,
                    tool_calls_count = response.tool_calls.len(),
                    finish_reason = %response.finish_reason,
                    "LLM response received"
                );

                // Check if there are no tool calls - this means final answer
                if response.tool_calls.is_empty() {
                    let final_response = response
                        .content
                        .unwrap_or_else(|| "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞, –Ω–æ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç.".to_string());

                    // Add assistant response to memory
                    self.session
                        .memory
                        .add_message(AgentMessage::assistant(&final_response));
                    self.session.complete();

                    info!(
                        task_id = %task_id,
                        iterations = iteration + 1,
                        "Agent task completed successfully"
                    );
                    return Ok(final_response);
                }

                // Add assistant message with tool calls placeholder
                // (We need to record that assistant requested tools)
                let tool_names: Vec<String> = response
                    .tool_calls
                    .iter()
                    .map(|tc| tc.function.name.clone())
                    .collect();
                messages.push(Message::assistant(&format!(
                    "[–í—ã–∑–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {}]",
                    tool_names.join(", ")
                )));

                // Ensure sandbox is running
                let sandbox = self
                    .session
                    .ensure_sandbox()
                    .await
                    .map_err(|e| anyhow!("Failed to create sandbox: {}", e))?;

                // Execute each tool call
                for tool_call in &response.tool_calls {
                    info!(
                        task_id = %task_id,
                        tool = %tool_call.function.name,
                        "Executing tool"
                    );

                    let result = execute_tool(
                        sandbox,
                        &tool_call.function.name,
                        &tool_call.function.arguments,
                    )
                    .await;

                    debug!(
                        task_id = %task_id,
                        tool = %tool_call.function.name,
                        result_len = result.len(),
                        "Tool execution completed"
                    );

                    // Add tool result to messages
                    messages.push(Message::tool(
                        &tool_call.id,
                        &tool_call.function.name,
                        &result,
                    ));
                }
            }

            // Max iterations reached
            self.session.fail("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π".to_string());
            Err(anyhow!(
                "–ê–≥–µ–Ω—Ç –ø—Ä–µ–≤—ã—Å–∏–ª –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π ({}). –í–æ–∑–º–æ–∂–Ω–æ, –∑–∞–¥–∞—á–∞ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è.",
                MAX_ITERATIONS
            ))
        })
        .await;

        match result {
            Ok(inner_result) => inner_result,
            Err(_) => {
                self.session.timeout();
                Err(anyhow!(
                    "–ó–∞–¥–∞—á–∞ –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –ª–∏–º–∏—Ç –≤—Ä–µ–º–µ–Ω–∏ ({} –º–∏–Ω—É—Ç)",
                    AGENT_TIMEOUT_SECS / 60
                ))
            }
        }
    }

    /// Run the agent loop with progress updates
    #[instrument(skip(llm_client, memory_messages, progress_tx), fields(task_id = %task_id))]
    async fn run_agent_loop(
        llm_client: Arc<LlmClient>,
        task: String,
        memory_messages: Vec<AgentMessage>,
        progress_tx: mpsc::Sender<ProgressUpdate>,
        task_id: String,
    ) -> Result<String> {
        debug!(task_id = %task_id, "Analyzing task via LLM");
        // Send initial progress
        let _ = progress_tx
            .send(ProgressUpdate {
                step: "üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–¥–∞—á—É...".to_string(),
                progress_percent: 10,
                is_final: false,
            })
            .await;

        let system_prompt = Self::create_agent_system_prompt();

        // Build LLM messages from memory
        let mut messages: Vec<crate::llm::Message> = Vec::new();
        for msg in &memory_messages {
            let role = match msg.role {
                super::memory::MessageRole::User => "user",
                super::memory::MessageRole::Assistant => "assistant",
                super::memory::MessageRole::System => "system",
            };
            messages.push(crate::llm::Message {
                role: role.to_string(),
                content: msg.content.clone(),
                tool_call_id: None,
                name: None,
            });
        }

        // Update progress
        let _ = progress_tx
            .send(ProgressUpdate {
                step: "üß† –í—ã–ø–æ–ª–Ω—è—é –∑–∞–¥–∞—á—É...".to_string(),
                progress_percent: 30,
                is_final: false,
            })
            .await;

        // Call the LLM
        let call_start = std::time::Instant::now();
        let response = llm_client
            .chat_completion(&system_prompt, &messages, &task, AGENT_MODEL)
            .await
            .map_err(|e| anyhow!("LLM call failed: {}", e))?;
        let call_duration = call_start.elapsed();

        debug!(
            task_id = %task_id,
            duration_ms = call_duration.as_millis(),
            "LLM call completed"
        );

        // Update progress before finalizing
        let _ = progress_tx
            .send(ProgressUpdate {
                step: "‚úÖ –§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç...".to_string(),
                progress_percent: 90,
                is_final: false,
            })
            .await;

        trace!(response = ?response, "LLM Response received");
        Ok(response)
    }

    /// Create the system prompt for the agent
    fn create_agent_system_prompt() -> String {
        r#"–¢—ã - AI-–∞–≥–µ–Ω—Ç —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (sandbox).

## –î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- **execute_command**: –≤—ã–ø–æ–ª–Ω–∏—Ç—å bash-–∫–æ–º–∞–Ω–¥—É –≤ sandbox (–¥–æ—Å—Ç—É–ø–Ω—ã: python3, pip, curl, wget, date, cat, ls, grep –∏ –¥—Ä—É–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã)
- **write_file**: –∑–∞–ø–∏—Å–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ —Ñ–∞–π–ª
- **read_file**: –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞

## –í–∞–∂–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞:
- –ï—Å–ª–∏ –Ω—É–∂–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–¥–∞—Ç–∞, –≤—Ä–µ–º—è, —Å–µ—Ç–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã) - –ò–°–ü–û–õ–¨–ó–£–ô –ò–ù–°–¢–†–£–ú–ï–ù–¢–´, –Ω–µ –æ–±—ä—è—Å–Ω—è–π –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å
- –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞ - –≤—ã–∑–æ–≤–∏ execute_command —Å –∫–æ–º–∞–Ω–¥–æ–π `date`
- –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–π Python: execute_command —Å `python3 -c "..."`
- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –±—É–¥—É—Ç –≤–æ–∑–≤—Ä–∞—â–µ–Ω—ã —Ç–µ–±–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ - –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –µ–≥–æ –∏ –¥–∞–π –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç

## –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–∫–æ–≥–¥–∞ –¥–∞—ë—à—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç):
- –ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —à–∞–≥–∏
- –î–∞–π —á—ë—Ç–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- –ò—Å–ø–æ–ª—å–∑—É–π markdown –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"#
            .to_string()
    }

    /// Cancel the current task
    pub fn cancel(&mut self) {
        self.session
            .fail("–ó–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º".to_string());
    }

    /// Reset the executor and session
    pub async fn reset(&mut self) {
        self.session.reset().await;
    }

    /// Check if the session is timed out
    pub fn is_timed_out(&self) -> bool {
        self.session.is_timed_out()
    }
}
